{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Agent Training and Experimentation\n",
        "\n",
        "This notebook provides a comprehensive training and experimentation environment for RL agents on portfolio optimization.\n",
        "\n",
        "## Objectives\n",
        "1. Train different RL agents (PPO, DQN) on portfolio environments\n",
        "2. Compare agent performance across different configurations\n",
        "3. Analyze training dynamics and convergence\n",
        "4. Evaluate agents against benchmark strategies\n",
        "5. Visualize results and performance metrics\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup and Imports\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import sys\n",
        "import os\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import json\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Add src to path\n",
        "sys.path.append(str(Path('..') / 'src'))\n",
        "\n",
        "# Import our modules\n",
        "from data.data_loader import load_and_prepare_data\n",
        "from environment.portfolio_env import create_portfolio_env\n",
        "from agents.ppo_agent import PPOAgent\n",
        "from agents.dqn_agent import DQNAgent\n",
        "\n",
        "# Set style\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "print(\"Libraries imported successfully!\")\n",
        "print(f\"Working directory: {os.getcwd()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Load and Prepare Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load data with different stock selections\n",
        "print(\"Loading data...\")\n",
        "\n",
        "# Simple selection for quick experiments\n",
        "prices_simple, returns_simple, quality_simple = load_and_prepare_data(\n",
        "    '../Files', stock_selection='simple'\n",
        ")\n",
        "\n",
        "# Diversified selection for comprehensive experiments\n",
        "prices_diversified, returns_diversified, quality_diversified = load_and_prepare_data(\n",
        "    '../Files', stock_selection='diversified'\n",
        ")\n",
        "\n",
        "print(f\"Simple dataset: {prices_simple.shape[0]} days, {prices_simple.shape[1]} stocks\")\n",
        "print(f\"Diversified dataset: {prices_diversified.shape[0]} days, {prices_diversified.shape[1]} stocks\")\n",
        "\n",
        "# Display data quality\n",
        "print(\"\\nData Quality Summary:\")\n",
        "print(f\"Simple - Quality Score: {quality_simple['data_quality_score']:.3f}\")\n",
        "print(f\"Diversified - Quality Score: {quality_diversified['data_quality_score']:.3f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Create Environments\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create different environment configurations\n",
        "print(\"Creating environments...\")\n",
        "\n",
        "# Simple environment for quick experiments\n",
        "env_simple = create_portfolio_env(\n",
        "    env_type='enhanced',\n",
        "    prices_df=prices_simple,\n",
        "    returns_df=returns_simple,\n",
        "    episode_length=252,  # 1 trading year\n",
        "    lookback_window=20,\n",
        "    transaction_cost=0.001\n",
        ")\n",
        "\n",
        "# Diversified environment for comprehensive experiments\n",
        "env_diversified = create_portfolio_env(\n",
        "    env_type='expanded',\n",
        "    prices_df=prices_diversified,\n",
        "    returns_df=returns_diversified,\n",
        "    episode_length=252,\n",
        "    lookback_window=20,\n",
        "    transaction_cost=0.001\n",
        ")\n",
        "\n",
        "print(f\"Simple Environment: {env_simple.__class__.__name__}\")\n",
        "print(f\"  State space: {env_simple.observation_space.shape}\")\n",
        "print(f\"  Action space: {env_simple.action_space.shape}\")\n",
        "\n",
        "print(f\"\\nDiversified Environment: {env_diversified.__class__.__name__}\")\n",
        "print(f\"  State space: {env_diversified.observation_space.shape}\")\n",
        "print(f\"  Action space: {env_diversified.action_space.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Agent Training Functions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_agent(env, agent, n_episodes=1000, save_freq=100, verbose=True):\n",
        "    \"\"\"\n",
        "    Train an RL agent on the portfolio environment.\n",
        "    \n",
        "    Args:\n",
        "        env: Portfolio environment\n",
        "        agent: RL agent to train\n",
        "        n_episodes: Number of training episodes\n",
        "        save_freq: Frequency of progress updates\n",
        "        verbose: Whether to print progress\n",
        "    \n",
        "    Returns:\n",
        "        Dictionary of training metrics\n",
        "    \"\"\"\n",
        "    print(f\"Training {agent.__class__.__name__} for {n_episodes} episodes...\")\n",
        "    \n",
        "    episode_rewards = []\n",
        "    episode_lengths = []\n",
        "    losses = []\n",
        "    \n",
        "    for episode in range(n_episodes):\n",
        "        state, info = env.reset()\n",
        "        episode_reward = 0\n",
        "        episode_length = 0\n",
        "        done = False\n",
        "        \n",
        "        while not done:\n",
        "            # Select action\n",
        "            action = agent.select_action(state, training=True)\n",
        "            \n",
        "            # Take step\n",
        "            next_state, reward, done, truncated, info = env.step(action)\n",
        "            \n",
        "            # Store experience (for agents that need it)\n",
        "            if hasattr(agent, 'store_reward'):\n",
        "                agent.store_reward(reward, done or truncated)\n",
        "            elif hasattr(agent, 'store_experience'):\n",
        "                agent.store_experience(state, action, reward, next_state, done or truncated)\n",
        "            \n",
        "            # Update agent\n",
        "            if episode_length % 10 == 0:  # Update every 10 steps\n",
        "                update_metrics = agent.update()\n",
        "                if update_metrics:\n",
        "                    losses.append(update_metrics.get('loss', 0))\n",
        "            \n",
        "            state = next_state\n",
        "            episode_reward += reward\n",
        "            episode_length += 1\n",
        "            \n",
        "            if truncated:\n",
        "                break\n",
        "        \n",
        "        # Store episode metrics\n",
        "        episode_rewards.append(episode_reward)\n",
        "        episode_lengths.append(episode_length)\n",
        "        \n",
        "        # Print progress\n",
        "        if verbose and episode % save_freq == 0:\n",
        "            avg_reward = np.mean(episode_rewards[-50:]) if len(episode_rewards) >= 50 else np.mean(episode_rewards)\n",
        "            avg_length = np.mean(episode_lengths[-50:]) if len(episode_lengths) >= 50 else np.mean(episode_lengths)\n",
        "            print(f\"Episode {episode:4d} | Avg Reward: {avg_reward:8.4f} | Avg Length: {avg_length:6.1f}\")\n",
        "    \n",
        "    return {\n",
        "        'episode_rewards': episode_rewards,\n",
        "        'episode_lengths': episode_lengths,\n",
        "        'losses': losses,\n",
        "        'n_episodes': n_episodes,\n",
        "        'agent_type': agent.__class__.__name__,\n",
        "        'env_type': env.__class__.__name__\n",
        "    }\n",
        "\n",
        "def evaluate_agent(env, agent, n_episodes=100):\n",
        "    \"\"\"\n",
        "    Evaluate a trained agent on the environment.\n",
        "    \n",
        "    Args:\n",
        "        env: Portfolio environment\n",
        "        agent: Trained RL agent\n",
        "        n_episodes: Number of evaluation episodes\n",
        "    \n",
        "    Returns:\n",
        "        Dictionary of evaluation metrics\n",
        "    \"\"\"\n",
        "    print(f\"Evaluating {agent.__class__.__name__} for {n_episodes} episodes...\")\n",
        "    \n",
        "    episode_rewards = []\n",
        "    episode_returns = []\n",
        "    episode_lengths = []\n",
        "    \n",
        "    for episode in range(n_episodes):\n",
        "        state, info = env.reset()\n",
        "        episode_reward = 0\n",
        "        episode_length = 0\n",
        "        done = False\n",
        "        \n",
        "        while not done:\n",
        "            # Select action (no training)\n",
        "            action = agent.select_action(state, training=False)\n",
        "            \n",
        "            # Take step\n",
        "            next_state, reward, done, truncated, info = env.step(action)\n",
        "            \n",
        "            state = next_state\n",
        "            episode_reward += reward\n",
        "            episode_length += 1\n",
        "            \n",
        "            if truncated:\n",
        "                break\n",
        "        \n",
        "        episode_rewards.append(episode_reward)\n",
        "        episode_returns.append(info['total_return'])\n",
        "        episode_lengths.append(episode_length)\n",
        "    \n",
        "    # Calculate metrics\n",
        "    metrics = {\n",
        "        'mean_reward': np.mean(episode_rewards),\n",
        "        'std_reward': np.std(episode_rewards),\n",
        "        'mean_return': np.mean(episode_returns),\n",
        "        'std_return': np.std(episode_returns),\n",
        "        'mean_length': np.mean(episode_lengths),\n",
        "        'std_length': np.std(episode_lengths),\n",
        "        'n_episodes': n_episodes\n",
        "    }\n",
        "    \n",
        "    print(f\"Evaluation Results:\")\n",
        "    print(f\"  Mean Reward: {metrics['mean_reward']:.4f} ± {metrics['std_reward']:.4f}\")\n",
        "    print(f\"  Mean Return: {metrics['mean_return']:.4f} ± {metrics['std_return']:.4f}\")\n",
        "    print(f\"  Mean Length: {metrics['mean_length']:.1f} ± {metrics['std_length']:.1f}\")\n",
        "    \n",
        "    return metrics\n",
        "\n",
        "print(\"Training functions defined successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Quick Training Experiments\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Quick training experiment with PPO on simple environment\n",
        "print(\"=== Quick PPO Training Experiment ===\")\n",
        "\n",
        "# Create PPO agent\n",
        "ppo_agent = PPOAgent(\n",
        "    state_dim=env_simple.observation_space.shape[0],\n",
        "    action_dim=env_simple.action_space.shape[0],\n",
        "    learning_rate=3e-4,\n",
        "    gamma=0.99,\n",
        "    clip_ratio=0.2,\n",
        "    ppo_epochs=4,\n",
        "    batch_size=64\n",
        ")\n",
        "\n",
        "# Train for a short period\n",
        "ppo_metrics = train_agent(env_simple, ppo_agent, n_episodes=200, save_freq=50)\n",
        "\n",
        "print(f\"\\nTraining completed!\")\n",
        "print(f\"Final average reward: {np.mean(ppo_metrics['episode_rewards'][-50:]):.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Training Visualization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot training metrics\n",
        "def plot_training_metrics(metrics, title=\"Training Metrics\"):\n",
        "    \"\"\"Plot training metrics.\"\"\"\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "    fig.suptitle(title, fontsize=16)\n",
        "    \n",
        "    # Episode rewards\n",
        "    axes[0, 0].plot(metrics['episode_rewards'])\n",
        "    axes[0, 0].set_title('Episode Rewards')\n",
        "    axes[0, 0].set_xlabel('Episode')\n",
        "    axes[0, 0].set_ylabel('Reward')\n",
        "    axes[0, 0].grid(True, alpha=0.3)\n",
        "    \n",
        "    # Moving average of rewards\n",
        "    window = min(20, len(metrics['episode_rewards']) // 10)\n",
        "    if window > 1:\n",
        "        moving_avg = pd.Series(metrics['episode_rewards']).rolling(window=window).mean()\n",
        "        axes[0, 1].plot(moving_avg)\n",
        "        axes[0, 1].set_title(f'Moving Average Rewards (window={window})')\n",
        "        axes[0, 1].set_xlabel('Episode')\n",
        "        axes[0, 1].set_ylabel('Average Reward')\n",
        "        axes[0, 1].grid(True, alpha=0.3)\n",
        "    \n",
        "    # Episode lengths\n",
        "    axes[1, 0].plot(metrics['episode_lengths'])\n",
        "    axes[1, 0].set_title('Episode Lengths')\n",
        "    axes[1, 0].set_xlabel('Episode')\n",
        "    axes[1, 0].set_ylabel('Length')\n",
        "    axes[1, 0].grid(True, alpha=0.3)\n",
        "    \n",
        "    # Losses (if available)\n",
        "    if metrics['losses']:\n",
        "        axes[1, 1].plot(metrics['losses'])\n",
        "        axes[1, 1].set_title('Training Loss')\n",
        "        axes[1, 1].set_xlabel('Update Step')\n",
        "        axes[1, 1].set_ylabel('Loss')\n",
        "        axes[1, 1].grid(True, alpha=0.3)\n",
        "    else:\n",
        "        axes[1, 1].text(0.5, 0.5, 'No loss data available', \n",
        "                       ha='center', va='center', transform=axes[1, 1].transAxes)\n",
        "        axes[1, 1].set_title('Training Loss')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Plot PPO training metrics\n",
        "plot_training_metrics(ppo_metrics, \"PPO Training Metrics\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Agent Comparison\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare PPO vs DQN on simple environment\n",
        "print(\"=== Agent Comparison ===\")\n",
        "\n",
        "# Create DQN agent\n",
        "dqn_agent = DQNAgent(\n",
        "    state_dim=env_simple.observation_space.shape[0],\n",
        "    action_dim=5,  # 5 discrete strategies\n",
        "    learning_rate=1e-4,\n",
        "    gamma=0.99,\n",
        "    epsilon_start=1.0,\n",
        "    epsilon_end=0.01,\n",
        "    epsilon_decay=0.995\n",
        ")\n",
        "\n",
        "# Train DQN agent\n",
        "dqn_metrics = train_agent(env_simple, dqn_agent, n_episodes=200, save_freq=50)\n",
        "\n",
        "print(f\"\\nDQN Training completed!\")\n",
        "print(f\"Final average reward: {np.mean(dqn_metrics['episode_rewards'][-50:]):.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare training curves\n",
        "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
        "fig.suptitle('Agent Comparison - Training Curves', fontsize=16)\n",
        "\n",
        "# Rewards comparison\n",
        "axes[0].plot(ppo_metrics['episode_rewards'], label='PPO', alpha=0.7)\n",
        "axes[0].plot(dqn_metrics['episode_rewards'], label='DQN', alpha=0.7)\n",
        "axes[0].set_title('Episode Rewards')\n",
        "axes[0].set_xlabel('Episode')\n",
        "axes[0].set_ylabel('Reward')\n",
        "axes[0].legend()\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "# Moving average comparison\n",
        "window = 20\n",
        "ppo_ma = pd.Series(ppo_metrics['episode_rewards']).rolling(window=window).mean()\n",
        "dqn_ma = pd.Series(dqn_metrics['episode_rewards']).rolling(window=window).mean()\n",
        "\n",
        "axes[1].plot(ppo_ma, label='PPO', alpha=0.7)\n",
        "axes[1].plot(dqn_ma, label='DQN', alpha=0.7)\n",
        "axes[1].set_title(f'Moving Average Rewards (window={window})')\n",
        "axes[1].set_xlabel('Episode')\n",
        "axes[1].set_ylabel('Average Reward')\n",
        "axes[1].legend()\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Print comparison summary\n",
        "print(\"\\n=== Training Summary ===\")\n",
        "print(f\"PPO - Final 50 episodes avg: {np.mean(ppo_metrics['episode_rewards'][-50:]):.4f}\")\n",
        "print(f\"DQN - Final 50 episodes avg: {np.mean(dqn_metrics['episode_rewards'][-50:]):.4f}\")\n",
        "print(f\"PPO - Max reward: {np.max(ppo_metrics['episode_rewards']):.4f}\")\n",
        "print(f\"DQN - Max reward: {np.max(dqn_metrics['episode_rewards']):.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Evaluation and Testing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate both agents\n",
        "print(\"=== Agent Evaluation ===\")\n",
        "\n",
        "# Evaluate PPO\n",
        "ppo_eval = evaluate_agent(env_simple, ppo_agent, n_episodes=50)\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "\n",
        "# Evaluate DQN\n",
        "dqn_eval = evaluate_agent(env_simple, dqn_agent, n_episodes=50)\n",
        "\n",
        "# Compare evaluation results\n",
        "print(\"\\n=== Evaluation Comparison ===\")\n",
        "print(f\"{'Metric':<20} {'PPO':<15} {'DQN':<15}\")\n",
        "print(\"-\" * 50)\n",
        "print(f\"{'Mean Reward':<20} {ppo_eval['mean_reward']:<15.4f} {dqn_eval['mean_reward']:<15.4f}\")\n",
        "print(f\"{'Mean Return':<20} {ppo_eval['mean_return']:<15.4f} {dqn_eval['mean_return']:<15.4f}\")\n",
        "print(f\"{'Mean Length':<20} {ppo_eval['mean_length']:<15.1f} {dqn_eval['mean_length']:<15.1f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Advanced Experiments\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Experiment with different hyperparameters\n",
        "print(\"=== Hyperparameter Experiment ===\")\n",
        "\n",
        "# Test different learning rates for PPO\n",
        "learning_rates = [1e-4, 3e-4, 1e-3]\n",
        "lr_results = {}\n",
        "\n",
        "for lr in learning_rates:\n",
        "    print(f\"\\nTesting learning rate: {lr}\")\n",
        "    \n",
        "    # Create agent with different learning rate\n",
        "    agent = PPOAgent(\n",
        "        state_dim=env_simple.observation_space.shape[0],\n",
        "        action_dim=env_simple.action_space.shape[0],\n",
        "        learning_rate=lr,\n",
        "        gamma=0.99,\n",
        "        clip_ratio=0.2,\n",
        "        ppo_epochs=4,\n",
        "        batch_size=64\n",
        "    )\n",
        "    \n",
        "    # Train for shorter period\n",
        "    metrics = train_agent(env_simple, agent, n_episodes=100, save_freq=50, verbose=False)\n",
        "    \n",
        "    # Store results\n",
        "    lr_results[lr] = {\n",
        "        'final_reward': np.mean(metrics['episode_rewards'][-20:]),\n",
        "        'max_reward': np.max(metrics['episode_rewards']),\n",
        "        'convergence': np.mean(metrics['episode_rewards'][-10:]) - np.mean(metrics['episode_rewards'][:10])\n",
        "    }\n",
        "    \n",
        "    print(f\"  Final reward: {lr_results[lr]['final_reward']:.4f}\")\n",
        "    print(f\"  Max reward: {lr_results[lr]['max_reward']:.4f}\")\n",
        "    print(f\"  Convergence: {lr_results[lr]['convergence']:.4f}\")\n",
        "\n",
        "# Plot learning rate comparison\n",
        "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
        "fig.suptitle('Learning Rate Comparison', fontsize=16)\n",
        "\n",
        "lrs = list(lr_results.keys())\n",
        "final_rewards = [lr_results[lr]['final_reward'] for lr in lrs]\n",
        "max_rewards = [lr_results[lr]['max_reward'] for lr in lrs]\n",
        "convergences = [lr_results[lr]['convergence'] for lr in lrs]\n",
        "\n",
        "axes[0].bar([str(lr) for lr in lrs], final_rewards, alpha=0.7)\n",
        "axes[0].set_title('Final Rewards')\n",
        "axes[0].set_xlabel('Learning Rate')\n",
        "axes[0].set_ylabel('Final Reward')\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "axes[1].bar([str(lr) for lr in lrs], max_rewards, alpha=0.7, color='orange')\n",
        "axes[1].set_title('Max Rewards')\n",
        "axes[1].set_xlabel('Learning Rate')\n",
        "axes[1].set_ylabel('Max Reward')\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "axes[2].bar([str(lr) for lr in lrs], convergences, alpha=0.7, color='green')\n",
        "axes[2].set_title('Convergence')\n",
        "axes[2].set_xlabel('Learning Rate')\n",
        "axes[2].set_ylabel('Convergence')\n",
        "axes[2].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Find best learning rate\n",
        "best_lr = max(lr_results.keys(), key=lambda x: lr_results[x]['final_reward'])\n",
        "print(f\"\\nBest learning rate: {best_lr} (Final reward: {lr_results[best_lr]['final_reward']:.4f})\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Save Results and Models\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save training results\n",
        "print(\"=== Saving Results ===\")\n",
        "\n",
        "# Create results directory\n",
        "results_dir = Path('../results')\n",
        "results_dir.mkdir(exist_ok=True)\n",
        "\n",
        "# Save training metrics\n",
        "with open(results_dir / 'ppo_training_metrics.json', 'w') as f:\n",
        "    json.dump(ppo_metrics, f, indent=2)\n",
        "\n",
        "with open(results_dir / 'dqn_training_metrics.json', 'w') as f:\n",
        "    json.dump(dqn_metrics, f, indent=2)\n",
        "\n",
        "# Save evaluation results\n",
        "with open(results_dir / 'ppo_evaluation.json', 'w') as f:\n",
        "    json.dump(ppo_eval, f, indent=2)\n",
        "\n",
        "with open(results_dir / 'dqn_evaluation.json', 'w') as f:\n",
        "    json.dump(dqn_eval, f, indent=2)\n",
        "\n",
        "# Save hyperparameter results\n",
        "with open(results_dir / 'hyperparameter_results.json', 'w') as f:\n",
        "    json.dump(lr_results, f, indent=2)\n",
        "\n",
        "print(f\"Results saved to: {results_dir}\")\n",
        "\n",
        "# Save trained models\n",
        "models_dir = Path('../models')\n",
        "models_dir.mkdir(exist_ok=True)\n",
        "\n",
        "ppo_agent.save(str(models_dir / 'ppo_agent.pth'))\n",
        "dqn_agent.save(str(models_dir / 'dqn_agent.pth'))\n",
        "\n",
        "print(f\"Models saved to: {models_dir}\")\n",
        "print(\"\\nTraining and evaluation completed successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Summary and Next Steps\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Print summary\n",
        "print(\"=== Training Summary ===\")\n",
        "print(f\"Environment: {env_simple.__class__.__name__}\")\n",
        "print(f\"State space: {env_simple.observation_space.shape}\")\n",
        "print(f\"Action space: {env_simple.action_space.shape}\")\n",
        "print(f\"Training episodes: {ppo_metrics['n_episodes']}\")\n",
        "print(f\"\\nBest performing agent: PPO\")\n",
        "print(f\"PPO final reward: {np.mean(ppo_metrics['episode_rewards'][-50:]):.4f}\")\n",
        "print(f\"DQN final reward: {np.mean(dqn_metrics['episode_rewards'][-50:]):.4f}\")\n",
        "print(f\"\\nBest learning rate: {best_lr}\")\n",
        "print(f\"\\nNext steps:\")\n",
        "print(f\"1. Train on diversified environment with more stocks\")\n",
        "print(f\"2. Implement more sophisticated reward functions\")\n",
        "print(f\"3. Add ensemble methods\")\n",
        "print(f\"4. Implement online learning and adaptation\")\n",
        "print(f\"5. Deploy for live trading (with proper risk management)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
