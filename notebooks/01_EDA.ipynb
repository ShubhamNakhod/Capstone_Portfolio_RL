{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Smart Portfolio Allocator - Exploratory Data Analysis\n",
        "\n",
        "## Overview\n",
        "This notebook performs exploratory data analysis on the stock market data for building a reinforcement learning-based portfolio allocation system.\n",
        "\n",
        "## Objectives\n",
        "1. Load and examine stock price data\n",
        "2. Calculate key financial metrics (returns, volatility, correlations)\n",
        "3. Visualize price trends and relationships\n",
        "4. Identify patterns and characteristics for RL environment design\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "from pathlib import Path\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set style for better plots\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "print(\"Libraries imported successfully!\")\n",
        "print(f\"Pandas version: {pd.__version__}\")\n",
        "print(f\"NumPy version: {np.__version__}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Data Loading\n",
        "\n",
        "First, let's load all the stock data from the Files directory.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define paths\n",
        "data_dir = Path(\"../Files\")\n",
        "print(f\"Data directory: {data_dir}\")\n",
        "print(f\"Directory exists: {data_dir.exists()}\")\n",
        "\n",
        "# List all CSV files\n",
        "csv_files = list(data_dir.glob(\"*.csv\"))\n",
        "print(f\"\\nFound {len(csv_files)} CSV files:\")\n",
        "for file in csv_files:\n",
        "    print(f\"  - {file.name}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function to load and process stock data\n",
        "def load_stock_data(file_path):\n",
        "    \"\"\"Load stock data from CSV file and standardize format.\"\"\"\n",
        "    try:\n",
        "        df = pd.read_csv(file_path)\n",
        "        \n",
        "        # Standardize column names (common variations)\n",
        "        column_mapping = {\n",
        "            'Date': 'date',\n",
        "            'DATE': 'date',\n",
        "            'Date/Time': 'date',\n",
        "            'Open': 'open',\n",
        "            'OPEN': 'open',\n",
        "            'High': 'high', \n",
        "            'HIGH': 'high',\n",
        "            'Low': 'low',\n",
        "            'LOW': 'low',\n",
        "            'Close': 'close',\n",
        "            'CLOSE': 'close',\n",
        "            'Adj Close': 'adj_close',\n",
        "            'Adj_Close': 'adj_close',\n",
        "            'Volume': 'volume',\n",
        "            'VOLUME': 'volume'\n",
        "        }\n",
        "        \n",
        "        df = df.rename(columns=column_mapping)\n",
        "        \n",
        "        # Convert date column\n",
        "        if 'date' in df.columns:\n",
        "            df['date'] = pd.to_datetime(df['date'])\n",
        "            df = df.set_index('date')\n",
        "        \n",
        "        return df\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"Error loading {file_path}: {e}\")\n",
        "        return None\n",
        "\n",
        "# Load all stock data\n",
        "stock_data = {}\n",
        "failed_files = []\n",
        "\n",
        "for file_path in csv_files:\n",
        "    stock_name = file_path.stem\n",
        "    df = load_stock_data(file_path)\n",
        "    \n",
        "    if df is not None:\n",
        "        stock_data[stock_name] = df\n",
        "        print(f\"âœ“ Loaded {stock_name}: {len(df)} rows, columns: {list(df.columns)}\")\n",
        "    else:\n",
        "        failed_files.append(stock_name)\n",
        "\n",
        "print(f\"\\nSuccessfully loaded {len(stock_data)} stocks\")\n",
        "if failed_files:\n",
        "    print(f\"Failed to load: {failed_files}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Data Overview\n",
        "\n",
        "Let's examine the structure and basic statistics of our loaded data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display basic information about each stock\n",
        "print(\"=== STOCK DATA OVERVIEW ===\\n\")\n",
        "\n",
        "for stock_name, df in stock_data.items():\n",
        "    print(f\"ðŸ“Š {stock_name.upper()}\")\n",
        "    print(f\"   Period: {df.index.min().strftime('%Y-%m-%d')} to {df.index.max().strftime('%Y-%m-%d')}\")\n",
        "    print(f\"   Data points: {len(df)}\")\n",
        "    print(f\"   Columns: {list(df.columns)}\")\n",
        "    print(f\"   Missing values: {df.isnull().sum().sum()}\")\n",
        "    print(f\"   Latest close price: ${df['close'].iloc[-1]:.2f}\" if 'close' in df.columns else \"   No close price data\")\n",
        "    print()\n",
        "\n",
        "# Check data consistency across stocks\n",
        "print(\"=== DATA CONSISTENCY CHECK ===\\n\")\n",
        "date_ranges = {}\n",
        "for stock_name, df in stock_data.items():\n",
        "    if len(df) > 0:\n",
        "        date_ranges[stock_name] = (df.index.min(), df.index.max())\n",
        "\n",
        "# Find common date range\n",
        "if date_ranges:\n",
        "    common_start = max([start for start, end in date_ranges.values()])\n",
        "    common_end = min([end for start, end in date_ranges.values()])\n",
        "    print(f\"Common trading period: {common_start.strftime('%Y-%m-%d')} to {common_end.strftime('%Y-%m-%d')}\")\n",
        "    \n",
        "    # Count stocks with data in common period\n",
        "    stocks_in_common_period = []\n",
        "    for stock_name, (start, end) in date_ranges.items():\n",
        "        if start <= common_start and end >= common_end:\n",
        "            stocks_in_common_period.append(stock_name)\n",
        "    \n",
        "    print(f\"Stocks with complete data in common period: {len(stocks_in_common_period)}\")\n",
        "    print(f\"Stocks: {', '.join(stocks_in_common_period[:10])}{'...' if len(stocks_in_common_period) > 10 else ''}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Price Analysis and Visualization\n",
        "\n",
        "Let's visualize the price trends for key stocks and analyze their movements.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a combined dataset for analysis\n",
        "def create_combined_dataset(stock_data, price_column='close'):\n",
        "    \"\"\"Create a combined dataset with all stock prices.\"\"\"\n",
        "    combined_data = {}\n",
        "    \n",
        "    for stock_name, df in stock_data.items():\n",
        "        if price_column in df.columns:\n",
        "            combined_data[stock_name] = df[price_column]\n",
        "    \n",
        "    if combined_data:\n",
        "        combined_df = pd.DataFrame(combined_data)\n",
        "        return combined_df\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "# Create combined dataset\n",
        "combined_prices = create_combined_dataset(stock_data)\n",
        "print(f\"Combined dataset shape: {combined_prices.shape}\")\n",
        "print(f\"Date range: {combined_prices.index.min()} to {combined_prices.index.max()}\")\n",
        "print(f\"Stocks included: {list(combined_prices.columns)}\")\n",
        "\n",
        "# Display first few rows\n",
        "print(\"\\nFirst 5 rows:\")\n",
        "print(combined_prices.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot price trends for selected stocks\n",
        "if combined_prices is not None and len(combined_prices.columns) > 0:\n",
        "    \n",
        "    # Select a subset of stocks for visualization (to avoid overcrowding)\n",
        "    n_stocks_to_plot = min(10, len(combined_prices.columns))\n",
        "    selected_stocks = combined_prices.columns[:n_stocks_to_plot]\n",
        "    \n",
        "    # Normalize prices to start at 100 for better comparison\n",
        "    normalized_prices = combined_prices[selected_stocks].copy()\n",
        "    for col in selected_stocks:\n",
        "        first_price = normalized_prices[col].dropna().iloc[0]\n",
        "        normalized_prices[col] = (normalized_prices[col] / first_price) * 100\n",
        "    \n",
        "    # Create the plot\n",
        "    plt.figure(figsize=(15, 8))\n",
        "    for stock in selected_stocks:\n",
        "        plt.plot(normalized_prices.index, normalized_prices[stock], \n",
        "                label=stock, linewidth=1.5, alpha=0.8)\n",
        "    \n",
        "    plt.title('Normalized Stock Price Trends (Starting at 100)', fontsize=16, fontweight='bold')\n",
        "    plt.xlabel('Date', fontsize=12)\n",
        "    plt.ylabel('Normalized Price', fontsize=12)\n",
        "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # Calculate and display basic statistics\n",
        "    print(\"\\n=== PRICE STATISTICS ===\")\n",
        "    print(\"Recent prices (last 5 days):\")\n",
        "    print(combined_prices[selected_stocks].tail())\n",
        "    \n",
        "    print(\"\\nPrice ranges:\")\n",
        "    price_stats = combined_prices[selected_stocks].describe()\n",
        "    print(price_stats)\n",
        "    \n",
        "else:\n",
        "    print(\"No price data available for visualization.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Returns Analysis\n",
        "\n",
        "Calculate and analyze returns, which are crucial for portfolio optimization and RL reward design.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate returns\n",
        "def calculate_returns(prices, method='simple'):\n",
        "    \"\"\"Calculate returns from price data.\"\"\"\n",
        "    if method == 'simple':\n",
        "        returns = prices.pct_change().dropna()\n",
        "    elif method == 'log':\n",
        "        returns = np.log(prices / prices.shift(1)).dropna()\n",
        "    else:\n",
        "        raise ValueError(\"Method must be 'simple' or 'log'\")\n",
        "    return returns\n",
        "\n",
        "# Calculate daily returns\n",
        "if combined_prices is not None:\n",
        "    daily_returns = calculate_returns(combined_prices, method='simple')\n",
        "    \n",
        "    print(f\"Daily returns shape: {daily_returns.shape}\")\n",
        "    print(f\"Date range: {daily_returns.index.min()} to {daily_returns.index.max()}\")\n",
        "    \n",
        "    # Display returns statistics\n",
        "    print(\"\\n=== RETURNS STATISTICS ===\")\n",
        "    returns_stats = daily_returns.describe()\n",
        "    print(returns_stats)\n",
        "    \n",
        "    # Check for extreme returns (potential data issues)\n",
        "    print(\"\\n=== EXTREME RETURNS CHECK ===\")\n",
        "    extreme_threshold = 0.5  # 50% daily return\n",
        "    extreme_returns = daily_returns[(daily_returns > extreme_threshold) | (daily_returns < -extreme_threshold)]\n",
        "    \n",
        "    if len(extreme_returns) > 0:\n",
        "        print(f\"Found {len(extreme_returns)} extreme returns (>50% or <-50%):\")\n",
        "        for idx, row in extreme_returns.iterrows():\n",
        "            for stock, ret in row.items():\n",
        "                if abs(ret) > extreme_threshold:\n",
        "                    print(f\"  {idx.strftime('%Y-%m-%d')} {stock}: {ret:.2%}\")\n",
        "    else:\n",
        "        print(\"No extreme returns found (>50% or <-50%)\")\n",
        "    \n",
        "    # Plot returns distribution\n",
        "    if len(daily_returns.columns) > 0:\n",
        "        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "        fig.suptitle('Returns Analysis', fontsize=16, fontweight='bold')\n",
        "        \n",
        "        # Select stocks for detailed analysis\n",
        "        analysis_stocks = daily_returns.columns[:4] if len(daily_returns.columns) >= 4 else daily_returns.columns\n",
        "        \n",
        "        for i, stock in enumerate(analysis_stocks):\n",
        "            row = i // 2\n",
        "            col = i % 2\n",
        "            \n",
        "            # Histogram of returns\n",
        "            axes[row, col].hist(daily_returns[stock].dropna(), bins=50, alpha=0.7, density=True)\n",
        "            axes[row, col].set_title(f'{stock} - Returns Distribution')\n",
        "            axes[row, col].set_xlabel('Daily Returns')\n",
        "            axes[row, col].set_ylabel('Density')\n",
        "            axes[row, col].grid(True, alpha=0.3)\n",
        "            \n",
        "            # Add statistics text\n",
        "            mean_ret = daily_returns[stock].mean()\n",
        "            std_ret = daily_returns[stock].std()\n",
        "            axes[row, col].text(0.05, 0.95, f'Mean: {mean_ret:.4f}\\nStd: {std_ret:.4f}', \n",
        "                              transform=axes[row, col].transAxes, verticalalignment='top',\n",
        "                              bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "        \n",
        "else:\n",
        "    print(\"No price data available for returns calculation.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Correlation Analysis\n",
        "\n",
        "Analyze correlations between stocks to understand diversification opportunities.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Correlation analysis\n",
        "if daily_returns is not None and len(daily_returns.columns) > 1:\n",
        "    \n",
        "    # Calculate correlation matrix\n",
        "    correlation_matrix = daily_returns.corr()\n",
        "    \n",
        "    print(\"=== CORRELATION ANALYSIS ===\")\n",
        "    print(f\"Correlation matrix shape: {correlation_matrix.shape}\")\n",
        "    \n",
        "    # Plot correlation heatmap\n",
        "    plt.figure(figsize=(12, 10))\n",
        "    mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))\n",
        "    sns.heatmap(correlation_matrix, mask=mask, annot=True, cmap='coolwarm', center=0,\n",
        "                square=True, fmt='.2f', cbar_kws={\"shrink\": .8})\n",
        "    plt.title('Stock Returns Correlation Matrix', fontsize=16, fontweight='bold')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # Find highly correlated pairs\n",
        "    print(\"\\n=== HIGHLY CORRELATED PAIRS ===\")\n",
        "    high_corr_threshold = 0.7\n",
        "    \n",
        "    high_corr_pairs = []\n",
        "    for i in range(len(correlation_matrix.columns)):\n",
        "        for j in range(i+1, len(correlation_matrix.columns)):\n",
        "            corr_value = correlation_matrix.iloc[i, j]\n",
        "            if abs(corr_value) > high_corr_threshold:\n",
        "                high_corr_pairs.append((correlation_matrix.columns[i], \n",
        "                                      correlation_matrix.columns[j], \n",
        "                                      corr_value))\n",
        "    \n",
        "    if high_corr_pairs:\n",
        "        print(f\"Found {len(high_corr_pairs)} pairs with correlation > {high_corr_threshold}:\")\n",
        "        for stock1, stock2, corr in sorted(high_corr_pairs, key=lambda x: abs(x[2]), reverse=True):\n",
        "            print(f\"  {stock1} - {stock2}: {corr:.3f}\")\n",
        "    else:\n",
        "        print(f\"No pairs found with correlation > {high_corr_threshold}\")\n",
        "    \n",
        "    # Calculate average correlation\n",
        "    avg_correlation = correlation_matrix.values[np.triu_indices_from(correlation_matrix.values, k=1)].mean()\n",
        "    print(f\"\\nAverage correlation: {avg_correlation:.3f}\")\n",
        "    \n",
        "    # Diversification analysis\n",
        "    print(\"\\n=== DIVERSIFICATION INSIGHTS ===\")\n",
        "    if avg_correlation < 0.3:\n",
        "        print(\"âœ“ Good diversification potential - low average correlation\")\n",
        "    elif avg_correlation < 0.6:\n",
        "        print(\"âš  Moderate diversification - some correlation exists\")\n",
        "    else:\n",
        "        print(\"âš  Limited diversification - high correlation between stocks\")\n",
        "        \n",
        "else:\n",
        "    print(\"Insufficient data for correlation analysis (need at least 2 stocks).\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Risk Metrics\n",
        "\n",
        "Calculate key risk metrics that will be important for the RL environment design.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate risk metrics\n",
        "def calculate_risk_metrics(returns):\n",
        "    \"\"\"Calculate various risk metrics for portfolio analysis.\"\"\"\n",
        "    metrics = {}\n",
        "    \n",
        "    # Annualized metrics (assuming 252 trading days)\n",
        "    trading_days = 252\n",
        "    \n",
        "    for stock in returns.columns:\n",
        "        stock_returns = returns[stock].dropna()\n",
        "        \n",
        "        if len(stock_returns) > 0:\n",
        "            # Basic metrics\n",
        "            mean_return = stock_returns.mean()\n",
        "            volatility = stock_returns.std()\n",
        "            \n",
        "            # Annualized metrics\n",
        "            annual_return = mean_return * trading_days\n",
        "            annual_volatility = volatility * np.sqrt(trading_days)\n",
        "            \n",
        "            # Sharpe ratio (assuming 2% risk-free rate)\n",
        "            risk_free_rate = 0.02\n",
        "            sharpe_ratio = (annual_return - risk_free_rate) / annual_volatility if annual_volatility > 0 else 0\n",
        "            \n",
        "            # Value at Risk (VaR) - 5th percentile\n",
        "            var_95 = np.percentile(stock_returns, 5)\n",
        "            \n",
        "            # Maximum Drawdown\n",
        "            cumulative_returns = (1 + stock_returns).cumprod()\n",
        "            running_max = cumulative_returns.expanding().max()\n",
        "            drawdown = (cumulative_returns - running_max) / running_max\n",
        "            max_drawdown = drawdown.min()\n",
        "            \n",
        "            metrics[stock] = {\n",
        "                'mean_daily_return': mean_return,\n",
        "                'daily_volatility': volatility,\n",
        "                'annual_return': annual_return,\n",
        "                'annual_volatility': annual_volatility,\n",
        "                'sharpe_ratio': sharpe_ratio,\n",
        "                'var_95': var_95,\n",
        "                'max_drawdown': max_drawdown\n",
        "            }\n",
        "    \n",
        "    return metrics\n",
        "\n",
        "# Calculate risk metrics\n",
        "if daily_returns is not None and len(daily_returns.columns) > 0:\n",
        "    risk_metrics = calculate_risk_metrics(daily_returns)\n",
        "    \n",
        "    print(\"=== RISK METRICS SUMMARY ===\")\n",
        "    print(f\"{'Stock':<10} {'Ann. Ret.':<10} {'Ann. Vol.':<10} {'Sharpe':<8} {'Max DD':<10} {'VaR 95%':<10}\")\n",
        "    print(\"-\" * 70)\n",
        "    \n",
        "    for stock, metrics in risk_metrics.items():\n",
        "        print(f\"{stock:<10} {metrics['annual_return']:<10.3f} {metrics['annual_volatility']:<10.3f} \"\n",
        "              f\"{metrics['sharpe_ratio']:<8.3f} {metrics['max_drawdown']:<10.3f} {metrics['var_95']:<10.3f}\")\n",
        "    \n",
        "    # Create risk-return scatter plot\n",
        "    if len(risk_metrics) > 1:\n",
        "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
        "        \n",
        "        # Risk-Return scatter plot\n",
        "        stocks = list(risk_metrics.keys())\n",
        "        returns_ann = [risk_metrics[stock]['annual_return'] for stock in stocks]\n",
        "        volatility_ann = [risk_metrics[stock]['annual_volatility'] for stock in stocks]\n",
        "        sharpe_ratios = [risk_metrics[stock]['sharpe_ratio'] for stock in stocks]\n",
        "        \n",
        "        scatter = ax1.scatter(volatility_ann, returns_ann, c=sharpe_ratios, \n",
        "                            cmap='viridis', s=100, alpha=0.7)\n",
        "        ax1.set_xlabel('Annual Volatility')\n",
        "        ax1.set_ylabel('Annual Return')\n",
        "        ax1.set_title('Risk-Return Profile')\n",
        "        ax1.grid(True, alpha=0.3)\n",
        "        \n",
        "        # Add stock labels\n",
        "        for i, stock in enumerate(stocks):\n",
        "            ax1.annotate(stock, (volatility_ann[i], returns_ann[i]), \n",
        "                        xytext=(5, 5), textcoords='offset points', fontsize=8)\n",
        "        \n",
        "        # Colorbar for Sharpe ratio\n",
        "        cbar = plt.colorbar(scatter, ax=ax1)\n",
        "        cbar.set_label('Sharpe Ratio')\n",
        "        \n",
        "        # Volatility comparison\n",
        "        ax2.bar(stocks, volatility_ann, alpha=0.7, color='skyblue')\n",
        "        ax2.set_xlabel('Stock')\n",
        "        ax2.set_ylabel('Annual Volatility')\n",
        "        ax2.set_title('Volatility Comparison')\n",
        "        ax2.tick_params(axis='x', rotation=45)\n",
        "        ax2.grid(True, alpha=0.3)\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "        \n",
        "        # Identify best and worst performers\n",
        "        print(\"\\n=== PERFORMANCE RANKINGS ===\")\n",
        "        \n",
        "        # Best Sharpe ratio\n",
        "        best_sharpe = max(risk_metrics.items(), key=lambda x: x[1]['sharpe_ratio'])\n",
        "        print(f\"Best Sharpe Ratio: {best_sharpe[0]} ({best_sharpe[1]['sharpe_ratio']:.3f})\")\n",
        "        \n",
        "        # Highest return\n",
        "        best_return = max(risk_metrics.items(), key=lambda x: x[1]['annual_return'])\n",
        "        print(f\"Highest Return: {best_return[0]} ({best_return[1]['annual_return']:.3f})\")\n",
        "        \n",
        "        # Lowest volatility\n",
        "        lowest_vol = min(risk_metrics.items(), key=lambda x: x[1]['annual_volatility'])\n",
        "        print(f\"Lowest Volatility: {lowest_vol[0]} ({lowest_vol[1]['annual_volatility']:.3f})\")\n",
        "        \n",
        "        # Best risk-adjusted (Sharpe > 1.0)\n",
        "        good_sharpe = [(stock, metrics['sharpe_ratio']) for stock, metrics in risk_metrics.items() \n",
        "                      if metrics['sharpe_ratio'] > 1.0]\n",
        "        if good_sharpe:\n",
        "            print(f\"\\nStocks with Sharpe Ratio > 1.0: {len(good_sharpe)}\")\n",
        "            for stock, sharpe in sorted(good_sharpe, key=lambda x: x[1], reverse=True):\n",
        "                print(f\"  {stock}: {sharpe:.3f}\")\n",
        "        else:\n",
        "            print(\"\\nNo stocks with Sharpe Ratio > 1.0\")\n",
        "            \n",
        "else:\n",
        "    print(\"No data available for risk metrics calculation.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. RL Environment Design Insights\n",
        "\n",
        "Based on our analysis, let's summarize key insights for designing the RL environment.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Summary and recommendations for RL environment design\n",
        "print(\"=== REINFORCEMENT LEARNING ENVIRONMENT DESIGN INSIGHTS ===\\n\")\n",
        "\n",
        "if combined_prices is not None and daily_returns is not None:\n",
        "    \n",
        "    # 1. State Space Design\n",
        "    print(\"1. STATE SPACE RECOMMENDATIONS:\")\n",
        "    print(f\"   â€¢ Number of assets: {len(combined_prices.columns)}\")\n",
        "    print(f\"   â€¢ Time period: {len(combined_prices)} days\")\n",
        "    print(\"   â€¢ State features to consider:\")\n",
        "    print(\"     - Current portfolio weights (normalized)\")\n",
        "    print(\"     - Recent price changes (e.g., last 5-20 days)\")\n",
        "    print(\"     - Technical indicators (RSI, MACD, moving averages)\")\n",
        "    print(\"     - Risk metrics (volatility, correlation)\")\n",
        "    print(\"     - Market regime indicators\")\n",
        "    \n",
        "    # 2. Action Space Design\n",
        "    print(\"\\n2. ACTION SPACE RECOMMENDATIONS:\")\n",
        "    print(\"   â€¢ Discrete actions: Buy/Hold/Sell for each asset\")\n",
        "    print(\"   â€¢ Continuous actions: Portfolio weight adjustments\")\n",
        "    print(\"   â€¢ Action constraints: Portfolio weights sum to 1.0\")\n",
        "    print(\"   â€¢ Transaction costs: Include in reward function\")\n",
        "    \n",
        "    # 3. Reward Function Design\n",
        "    print(\"\\n3. REWARD FUNCTION RECOMMENDATIONS:\")\n",
        "    print(\"   â€¢ Primary: Risk-adjusted returns (Sharpe ratio)\")\n",
        "    print(\"   â€¢ Secondary: Portfolio return, volatility penalty\")\n",
        "    print(\"   â€¢ Transaction costs: Penalty for frequent rebalancing\")\n",
        "    print(\"   â€¢ Drawdown penalty: Avoid large losses\")\n",
        "    \n",
        "    # 4. Data Preprocessing\n",
        "    print(\"\\n4. DATA PREPROCESSING RECOMMENDATIONS:\")\n",
        "    print(\"   â€¢ Normalize prices to returns for stationarity\")\n",
        "    print(\"   â€¢ Handle missing values (forward fill or interpolation)\")\n",
        "    print(\"   â€¢ Create train/validation/test splits\")\n",
        "    print(\"   â€¢ Consider rolling window approach for training\")\n",
        "    \n",
        "    # 5. Environment Parameters\n",
        "    print(\"\\n5. ENVIRONMENT PARAMETERS:\")\n",
        "    print(f\"   â€¢ Episode length: {min(252, len(combined_prices)//10)} days (1 trading year)\")\n",
        "    print(\"   â€¢ Initial portfolio: Equal weights or cash\")\n",
        "    print(\"   â€¢ Rebalancing frequency: Daily or weekly\")\n",
        "    print(\"   â€¢ Transaction costs: 0.1% - 0.5% per trade\")\n",
        "    \n",
        "    # 6. Model Selection\n",
        "    print(\"\\n6. RL ALGORITHM RECOMMENDATIONS:\")\n",
        "    print(\"   â€¢ DQN: Good for discrete action spaces\")\n",
        "    print(\"   â€¢ PPO/A3C: Better for continuous action spaces\")\n",
        "    print(\"   â€¢ SAC/TD3: Advanced continuous control\")\n",
        "    print(\"   â€¢ Consider ensemble methods for robustness\")\n",
        "    \n",
        "    # 7. Validation Strategy\n",
        "    print(\"\\n7. VALIDATION STRATEGY:\")\n",
        "    print(\"   â€¢ Time-series split (no shuffle)\")\n",
        "    print(\"   â€¢ Walk-forward analysis\")\n",
        "    print(\"   â€¢ Out-of-sample testing\")\n",
        "    print(\"   â€¢ Compare against benchmark (equal weights, buy & hold)\")\n",
        "    \n",
        "else:\n",
        "    print(\"Insufficient data for RL environment design recommendations.\")\n",
        "    print(\"Please ensure stock data is properly loaded before proceeding.\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"NEXT STEPS:\")\n",
        "print(\"1. Implement the RL environment class\")\n",
        "print(\"2. Choose and implement RL algorithm\")\n",
        "print(\"3. Design reward function based on insights\")\n",
        "print(\"4. Train and validate the model\")\n",
        "print(\"5. Backtest on historical data\")\n",
        "print(\"=\"*60)\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
