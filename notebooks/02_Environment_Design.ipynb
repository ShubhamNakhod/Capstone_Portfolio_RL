{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Smart Portfolio Allocator - RL Environment Design & Prototyping\n",
        "\n",
        "## Overview\n",
        "This notebook focuses on designing and prototyping the reinforcement learning environment for portfolio optimization. We'll experiment with different state representations, action spaces, and reward functions to find the optimal configuration.\n",
        "\n",
        "## Objectives\n",
        "1. Design state space representations\n",
        "2. Prototype action spaces (discrete vs continuous)\n",
        "3. Test different reward functions\n",
        "4. Validate environment with sample episodes\n",
        "5. Optimize environment parameters\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from pathlib import Path\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# RL and ML libraries\n",
        "import gymnasium as gym\n",
        "from gymnasium import spaces\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from stable_baselines3 import DQN, PPO, A2C\n",
        "from stable_baselines3.common.env_checker import check_env\n",
        "\n",
        "# Set style and seed\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "\n",
        "print(\"Libraries imported successfully!\")\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"Gymnasium version: {gym.__version__}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Load and Prepare Data\n",
        "\n",
        "First, let's load the stock data and prepare it for the RL environment.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load stock data (reusing the function from EDA)\n",
        "def load_stock_data(file_path):\n",
        "    \"\"\"Load stock data from CSV file and standardize format.\"\"\"\n",
        "    try:\n",
        "        df = pd.read_csv(file_path)\n",
        "        \n",
        "        # Standardize column names\n",
        "        column_mapping = {\n",
        "            'Date': 'date', 'DATE': 'date', 'Date/Time': 'date',\n",
        "            'Open': 'open', 'OPEN': 'open',\n",
        "            'High': 'high', 'HIGH': 'high',\n",
        "            'Low': 'low', 'LOW': 'low',\n",
        "            'Close': 'close', 'CLOSE': 'close',\n",
        "            'Adj Close': 'adj_close', 'Adj_Close': 'adj_close',\n",
        "            'Volume': 'volume', 'VOLUME': 'volume'\n",
        "        }\n",
        "        \n",
        "        df = df.rename(columns=column_mapping)\n",
        "        \n",
        "        # Convert date column\n",
        "        if 'date' in df.columns:\n",
        "            df['date'] = pd.to_datetime(df['date'])\n",
        "            df = df.set_index('date')\n",
        "        \n",
        "        return df\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"Error loading {file_path}: {e}\")\n",
        "        return None\n",
        "\n",
        "# Load all stock data\n",
        "data_dir = Path(\"../Files\")\n",
        "csv_files = list(data_dir.glob(\"*.csv\"))\n",
        "\n",
        "# Select a subset of stocks for prototyping (to keep environment manageable)\n",
        "selected_stocks = ['AAPL', 'AMZN', 'GOOG', 'MSFT', 'JNJ', 'SP500']  # Tech + Healthcare + Benchmark\n",
        "\n",
        "stock_data = {}\n",
        "for file_path in csv_files:\n",
        "    stock_name = file_path.stem\n",
        "    if stock_name in selected_stocks:\n",
        "        df = load_stock_data(file_path)\n",
        "        if df is not None and 'close' in df.columns:\n",
        "            stock_data[stock_name] = df\n",
        "            print(f\"âœ“ Loaded {stock_name}: {len(df)} rows\")\n",
        "\n",
        "print(f\"\\nLoaded {len(stock_data)} stocks for prototyping: {list(stock_data.keys())}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create combined dataset and calculate returns\n",
        "def create_combined_dataset(stock_data, price_column='close'):\n",
        "    \"\"\"Create a combined dataset with all stock prices.\"\"\"\n",
        "    combined_data = {}\n",
        "    \n",
        "    for stock_name, df in stock_data.items():\n",
        "        if price_column in df.columns:\n",
        "            combined_data[stock_name] = df[price_column]\n",
        "    \n",
        "    if combined_data:\n",
        "        combined_df = pd.DataFrame(combined_data)\n",
        "        return combined_df\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "# Create combined dataset\n",
        "combined_prices = create_combined_dataset(stock_data)\n",
        "print(f\"Combined dataset shape: {combined_prices.shape}\")\n",
        "print(f\"Date range: {combined_prices.index.min()} to {combined_prices.index.max()}\")\n",
        "\n",
        "# Calculate returns\n",
        "daily_returns = combined_prices.pct_change().dropna()\n",
        "print(f\"Returns dataset shape: {daily_returns.shape}\")\n",
        "\n",
        "# Display basic statistics\n",
        "print(\"\\n=== DATA STATISTICS ===\")\n",
        "print(\"Recent prices:\")\n",
        "print(combined_prices.tail(3))\n",
        "print(\"\\nRecent returns:\")\n",
        "print(daily_returns.tail(3))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Base Portfolio Environment Class\n",
        "\n",
        "Let's create a base environment class that we can extend and modify for different prototypes.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class BasePortfolioEnv(gym.Env):\n",
        "    \"\"\"\n",
        "    Base Portfolio Environment for Reinforcement Learning\n",
        "    \n",
        "    This is a prototype environment that can be extended with different\n",
        "    state representations, action spaces, and reward functions.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, prices_df, returns_df, initial_capital=10000, \n",
        "                 transaction_cost=0.001, episode_length=252, \n",
        "                 lookback_window=20):\n",
        "        super().__init__()\n",
        "        \n",
        "        # Data\n",
        "        self.prices_df = prices_df\n",
        "        self.returns_df = returns_df\n",
        "        self.stock_names = list(prices_df.columns)\n",
        "        self.n_stocks = len(self.stock_names)\n",
        "        \n",
        "        # Environment parameters\n",
        "        self.initial_capital = initial_capital\n",
        "        self.transaction_cost = transaction_cost\n",
        "        self.episode_length = episode_length\n",
        "        self.lookback_window = lookback_window\n",
        "        \n",
        "        # Episode state\n",
        "        self.current_step = 0\n",
        "        self.current_portfolio_value = initial_capital\n",
        "        self.portfolio_weights = np.ones(self.n_stocks) / self.n_stocks  # Equal weights initially\n",
        "        self.cash_weight = 0.0\n",
        "        \n",
        "        # Track performance\n",
        "        self.portfolio_history = []\n",
        "        self.weight_history = []\n",
        "        self.return_history = []\n",
        "        \n",
        "        # Define action and observation spaces (to be overridden in subclasses)\n",
        "        self.action_space = None  # Will be defined in subclasses\n",
        "        self.observation_space = None  # Will be defined in subclasses\n",
        "        \n",
        "    def reset(self, seed=None, options=None):\n",
        "        \"\"\"Reset the environment to initial state\"\"\"\n",
        "        super().reset(seed=seed)\n",
        "        \n",
        "        # Reset episode state\n",
        "        self.current_step = self.lookback_window  # Start after lookback window\n",
        "        self.current_portfolio_value = self.initial_capital\n",
        "        self.portfolio_weights = np.ones(self.n_stocks) / self.n_stocks\n",
        "        self.cash_weight = 0.0\n",
        "        \n",
        "        # Clear history\n",
        "        self.portfolio_history = []\n",
        "        self.weight_history = []\n",
        "        self.return_history = []\n",
        "        \n",
        "        # Get initial observation\n",
        "        observation = self._get_observation()\n",
        "        info = self._get_info()\n",
        "        \n",
        "        return observation, info\n",
        "    \n",
        "    def step(self, action):\n",
        "        \"\"\"Execute one step in the environment\"\"\"\n",
        "        # Execute action (to be implemented in subclasses)\n",
        "        self._execute_action(action)\n",
        "        \n",
        "        # Calculate reward (to be implemented in subclasses)\n",
        "        reward = self._calculate_reward()\n",
        "        \n",
        "        # Update portfolio value\n",
        "        self._update_portfolio_value()\n",
        "        \n",
        "        # Store history\n",
        "        self.portfolio_history.append(self.current_portfolio_value)\n",
        "        self.weight_history.append(self.portfolio_weights.copy())\n",
        "        \n",
        "        # Check if episode is done\n",
        "        self.current_step += 1\n",
        "        done = self.current_step >= len(self.returns_df) - 1\n",
        "        \n",
        "        # Get next observation\n",
        "        observation = self._get_observation() if not done else None\n",
        "        info = self._get_info()\n",
        "        \n",
        "        return observation, reward, done, False, info\n",
        "    \n",
        "    def _get_observation(self):\n",
        "        \"\"\"Get current observation (to be implemented in subclasses)\"\"\"\n",
        "        raise NotImplementedError(\"Subclasses must implement _get_observation\")\n",
        "    \n",
        "    def _execute_action(self, action):\n",
        "        \"\"\"Execute action (to be implemented in subclasses)\"\"\"\n",
        "        raise NotImplementedError(\"Subclasses must implement _execute_action\")\n",
        "    \n",
        "    def _calculate_reward(self):\n",
        "        \"\"\"Calculate reward (to be implemented in subclasses)\"\"\"\n",
        "        raise NotImplementedError(\"Subclasses must implement _calculate_reward\")\n",
        "    \n",
        "    def _update_portfolio_value(self):\n",
        "        \"\"\"Update portfolio value based on returns\"\"\"\n",
        "        if self.current_step < len(self.returns_df):\n",
        "            # Calculate portfolio return\n",
        "            current_returns = self.returns_df.iloc[self.current_step].values\n",
        "            portfolio_return = np.sum(self.portfolio_weights * current_returns)\n",
        "            \n",
        "            # Update portfolio value\n",
        "            self.current_portfolio_value *= (1 + portfolio_return)\n",
        "            self.return_history.append(portfolio_return)\n",
        "    \n",
        "    def _get_info(self):\n",
        "        \"\"\"Get additional information\"\"\"\n",
        "        return {\n",
        "            'portfolio_value': self.current_portfolio_value,\n",
        "            'portfolio_weights': self.portfolio_weights.copy(),\n",
        "            'current_step': self.current_step,\n",
        "            'total_return': (self.current_portfolio_value / self.initial_capital) - 1\n",
        "        }\n",
        "    \n",
        "    def render(self, mode='human'):\n",
        "        \"\"\"Render the environment\"\"\"\n",
        "        if mode == 'human':\n",
        "            print(f\"Step: {self.current_step}\")\n",
        "            print(f\"Portfolio Value: ${self.current_portfolio_value:.2f}\")\n",
        "            print(f\"Total Return: {((self.current_portfolio_value / self.initial_capital) - 1) * 100:.2f}%\")\n",
        "            print(f\"Portfolio Weights: {dict(zip(self.stock_names, self.portfolio_weights))}\")\n",
        "\n",
        "print(\"Base Portfolio Environment class defined successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. State Space Design Prototypes\n",
        "\n",
        "Let's create different state space representations to test which works best for portfolio optimization.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class PortfolioEnvV1(BasePortfolioEnv):\n",
        "    \"\"\"\n",
        "    Prototype 1: Simple State Space\n",
        "    State: [current_portfolio_weights, recent_returns, portfolio_value_ratio]\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, prices_df, returns_df, **kwargs):\n",
        "        super().__init__(prices_df, returns_df, **kwargs)\n",
        "        \n",
        "        # State space: portfolio weights + recent returns + portfolio value ratio\n",
        "        state_dim = self.n_stocks + self.lookback_window + 1\n",
        "        self.observation_space = spaces.Box(\n",
        "            low=-np.inf, high=np.inf, shape=(state_dim,), dtype=np.float32\n",
        "        )\n",
        "        \n",
        "        # Action space: discrete actions for each stock (buy/hold/sell)\n",
        "        self.action_space = spaces.MultiDiscrete([3] * self.n_stocks)  # 0=sell, 1=hold, 2=buy\n",
        "    \n",
        "    def _get_observation(self):\n",
        "        \"\"\"Get observation for V1\"\"\"\n",
        "        obs = []\n",
        "        \n",
        "        # Current portfolio weights\n",
        "        obs.extend(self.portfolio_weights)\n",
        "        \n",
        "        # Recent returns (lookback window)\n",
        "        if self.current_step >= self.lookback_window:\n",
        "            recent_returns = self.returns_df.iloc[\n",
        "                self.current_step - self.lookback_window:self.current_step\n",
        "            ].mean().values\n",
        "            obs.extend(recent_returns)\n",
        "        else:\n",
        "            obs.extend(np.zeros(self.lookback_window))\n",
        "        \n",
        "        # Portfolio value ratio (current/initial)\n",
        "        portfolio_ratio = self.current_portfolio_value / self.initial_capital\n",
        "        obs.append(portfolio_ratio)\n",
        "        \n",
        "        return np.array(obs, dtype=np.float32)\n",
        "    \n",
        "    def _execute_action(self, action):\n",
        "        \"\"\"Execute action for V1\"\"\"\n",
        "        # Convert discrete actions to portfolio weight changes\n",
        "        action_weights = np.array(action) - 1  # Convert 0,1,2 to -1,0,1\n",
        "        \n",
        "        # Normalize action weights to sum to 0 (no net change in total allocation)\n",
        "        action_weights = action_weights / np.sum(np.abs(action_weights)) if np.sum(np.abs(action_weights)) > 0 else action_weights\n",
        "        \n",
        "        # Update portfolio weights with transaction costs\n",
        "        weight_change = action_weights * 0.1  # 10% adjustment per action\n",
        "        new_weights = self.portfolio_weights + weight_change\n",
        "        \n",
        "        # Ensure weights are non-negative and sum to 1\n",
        "        new_weights = np.maximum(new_weights, 0)\n",
        "        new_weights = new_weights / np.sum(new_weights)\n",
        "        \n",
        "        self.portfolio_weights = new_weights\n",
        "    \n",
        "    def _calculate_reward(self):\n",
        "        \"\"\"Calculate reward for V1\"\"\"\n",
        "        if self.current_step == 0:\n",
        "            return 0\n",
        "        \n",
        "        # Simple reward: portfolio return\n",
        "        portfolio_return = self.return_history[-1] if self.return_history else 0\n",
        "        \n",
        "        # Add small penalty for large weight changes (transaction costs)\n",
        "        if len(self.weight_history) > 1:\n",
        "            weight_change = np.sum(np.abs(self.portfolio_weights - self.weight_history[-1]))\n",
        "            transaction_penalty = weight_change * self.transaction_cost\n",
        "        else:\n",
        "            transaction_penalty = 0\n",
        "        \n",
        "        reward = portfolio_return - transaction_penalty\n",
        "        return reward\n",
        "\n",
        "class PortfolioEnvV2(BasePortfolioEnv):\n",
        "    \"\"\"\n",
        "    Prototype 2: Technical Indicators State Space\n",
        "    State: [portfolio_weights, price_ratios, volatility, momentum, rsi_indicators]\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, prices_df, returns_df, **kwargs):\n",
        "        super().__init__(prices_df, returns_df, **kwargs)\n",
        "        \n",
        "        # Calculate technical indicators\n",
        "        self._calculate_technical_indicators()\n",
        "        \n",
        "        # State space: portfolio weights + technical indicators\n",
        "        state_dim = self.n_stocks + (self.n_stocks * 4)  # weights + 4 indicators per stock\n",
        "        self.observation_space = spaces.Box(\n",
        "            low=-np.inf, high=np.inf, shape=(state_dim,), dtype=np.float32\n",
        "        )\n",
        "        \n",
        "        # Action space: continuous portfolio weights\n",
        "        self.action_space = spaces.Box(\n",
        "            low=0, high=1, shape=(self.n_stocks,), dtype=np.float32\n",
        "        )\n",
        "    \n",
        "    def _calculate_technical_indicators(self):\n",
        "        \"\"\"Calculate technical indicators for all stocks\"\"\"\n",
        "        self.price_ratios = {}\n",
        "        self.volatility = {}\n",
        "        self.momentum = {}\n",
        "        self.rsi = {}\n",
        "        \n",
        "        for stock in self.stock_names:\n",
        "            prices = self.prices_df[stock].values\n",
        "            \n",
        "            # Price ratios (current/20-day average)\n",
        "            self.price_ratios[stock] = prices / pd.Series(prices).rolling(20).mean().values\n",
        "            \n",
        "            # Volatility (20-day rolling std)\n",
        "            self.volatility[stock] = pd.Series(self.returns_df[stock]).rolling(20).std().values\n",
        "            \n",
        "            # Momentum (5-day return)\n",
        "            self.momentum[stock] = self.returns_df[stock].rolling(5).sum().values\n",
        "            \n",
        "            # Simple RSI approximation\n",
        "            returns = self.returns_df[stock].values\n",
        "            gains = np.where(returns > 0, returns, 0)\n",
        "            losses = np.where(returns < 0, -returns, 0)\n",
        "            avg_gain = pd.Series(gains).rolling(14).mean().values\n",
        "            avg_loss = pd.Series(losses).rolling(14).mean().values\n",
        "            rs = avg_gain / (avg_loss + 1e-8)\n",
        "            self.rsi[stock] = 100 - (100 / (1 + rs))\n",
        "    \n",
        "    def _get_observation(self):\n",
        "        \"\"\"Get observation for V2\"\"\"\n",
        "        obs = []\n",
        "        \n",
        "        # Current portfolio weights\n",
        "        obs.extend(self.portfolio_weights)\n",
        "        \n",
        "        # Technical indicators for current step\n",
        "        for stock in self.stock_names:\n",
        "            if self.current_step < len(self.prices_df):\n",
        "                obs.append(self.price_ratios[stock][self.current_step])\n",
        "                obs.append(self.volatility[stock][self.current_step])\n",
        "                obs.append(self.momentum[stock][self.current_step])\n",
        "                obs.append(self.rsi[stock][self.current_step])\n",
        "            else:\n",
        "                obs.extend([1.0, 0.0, 0.0, 50.0])  # Default values\n",
        "        \n",
        "        return np.array(obs, dtype=np.float32)\n",
        "    \n",
        "    def _execute_action(self, action):\n",
        "        \"\"\"Execute action for V2\"\"\"\n",
        "        # Direct portfolio weight assignment with normalization\n",
        "        new_weights = np.array(action)\n",
        "        new_weights = np.maximum(new_weights, 0)  # Ensure non-negative\n",
        "        new_weights = new_weights / np.sum(new_weights)  # Normalize to sum to 1\n",
        "        \n",
        "        self.portfolio_weights = new_weights\n",
        "    \n",
        "    def _calculate_reward(self):\n",
        "        \"\"\"Calculate reward for V2\"\"\"\n",
        "        if self.current_step == 0:\n",
        "            return 0\n",
        "        \n",
        "        # Portfolio return\n",
        "        portfolio_return = self.return_history[-1] if self.return_history else 0\n",
        "        \n",
        "        # Risk-adjusted reward (Sharpe-like)\n",
        "        if len(self.return_history) > 10:\n",
        "            recent_returns = np.array(self.return_history[-10:])\n",
        "            sharpe_ratio = np.mean(recent_returns) / (np.std(recent_returns) + 1e-8)\n",
        "            reward = sharpe_ratio * 0.1  # Scale down\n",
        "        else:\n",
        "            reward = portfolio_return\n",
        "        \n",
        "        return reward\n",
        "\n",
        "print(\"State space prototypes V1 and V2 created successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Action Space Prototypes\n",
        "\n",
        "Let's create different action space designs to test discrete vs continuous approaches.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class PortfolioEnvV3(BasePortfolioEnv):\n",
        "    \"\"\"\n",
        "    Prototype 3: Discrete Action Space with Position Sizing\n",
        "    Actions: [0=25%, 1=50%, 2=75%, 3=100%] allocation per stock\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, prices_df, returns_df, **kwargs):\n",
        "        super().__init__(prices_df, returns_df, **kwargs)\n",
        "        \n",
        "        # State space: simple - just portfolio weights and recent returns\n",
        "        state_dim = self.n_stocks + self.lookback_window\n",
        "        self.observation_space = spaces.Box(\n",
        "            low=-np.inf, high=np.inf, shape=(state_dim,), dtype=np.float32\n",
        "        )\n",
        "        \n",
        "        # Action space: discrete allocation levels for each stock\n",
        "        self.action_space = spaces.MultiDiscrete([4] * self.n_stocks)  # 0=25%, 1=50%, 2=75%, 3=100%\n",
        "    \n",
        "    def _get_observation(self):\n",
        "        \"\"\"Get observation for V3\"\"\"\n",
        "        obs = []\n",
        "        \n",
        "        # Current portfolio weights\n",
        "        obs.extend(self.portfolio_weights)\n",
        "        \n",
        "        # Recent returns (lookback window)\n",
        "        if self.current_step >= self.lookback_window:\n",
        "            recent_returns = self.returns_df.iloc[\n",
        "                self.current_step - self.lookback_window:self.current_step\n",
        "            ].mean().values\n",
        "            obs.extend(recent_returns)\n",
        "        else:\n",
        "            obs.extend(np.zeros(self.lookback_window))\n",
        "        \n",
        "        return np.array(obs, dtype=np.float32)\n",
        "    \n",
        "    def _execute_action(self, action):\n",
        "        \"\"\"Execute action for V3\"\"\"\n",
        "        # Convert discrete actions to portfolio weights\n",
        "        allocation_levels = [0.25, 0.5, 0.75, 1.0]\n",
        "        \n",
        "        # Calculate target weights based on actions\n",
        "        target_weights = np.array([allocation_levels[a] for a in action])\n",
        "        \n",
        "        # Normalize to sum to 1\n",
        "        target_weights = target_weights / np.sum(target_weights)\n",
        "        \n",
        "        # Apply transaction costs for rebalancing\n",
        "        weight_change = np.sum(np.abs(target_weights - self.portfolio_weights))\n",
        "        transaction_cost_penalty = weight_change * self.transaction_cost\n",
        "        \n",
        "        # Update weights\n",
        "        self.portfolio_weights = target_weights\n",
        "        \n",
        "        # Store transaction cost for reward calculation\n",
        "        self.last_transaction_cost = transaction_cost_penalty\n",
        "    \n",
        "    def _calculate_reward(self):\n",
        "        \"\"\"Calculate reward for V3\"\"\"\n",
        "        if self.current_step == 0:\n",
        "            return 0\n",
        "        \n",
        "        # Portfolio return\n",
        "        portfolio_return = self.return_history[-1] if self.return_history else 0\n",
        "        \n",
        "        # Subtract transaction costs\n",
        "        transaction_penalty = getattr(self, 'last_transaction_cost', 0)\n",
        "        \n",
        "        reward = portfolio_return - transaction_penalty\n",
        "        return reward\n",
        "\n",
        "class PortfolioEnvV4(BasePortfolioEnv):\n",
        "    \"\"\"\n",
        "    Prototype 4: Continuous Action Space with Softmax\n",
        "    Actions: Raw logits that get converted to portfolio weights via softmax\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, prices_df, returns_df, **kwargs):\n",
        "        super().__init__(prices_df, returns_df, **kwargs)\n",
        "        \n",
        "        # State space: portfolio weights + technical indicators\n",
        "        state_dim = self.n_stocks * 3  # weights + returns + volatility\n",
        "        self.observation_space = spaces.Box(\n",
        "            low=-np.inf, high=np.inf, shape=(state_dim,), dtype=np.float32\n",
        "        )\n",
        "        \n",
        "        # Action space: continuous logits for each stock\n",
        "        self.action_space = spaces.Box(\n",
        "            low=-10, high=10, shape=(self.n_stocks,), dtype=np.float32\n",
        "        )\n",
        "    \n",
        "    def _get_observation(self):\n",
        "        \"\"\"Get observation for V4\"\"\"\n",
        "        obs = []\n",
        "        \n",
        "        # Current portfolio weights\n",
        "        obs.extend(self.portfolio_weights)\n",
        "        \n",
        "        # Recent returns\n",
        "        if self.current_step > 0:\n",
        "            recent_returns = self.returns_df.iloc[self.current_step].values\n",
        "        else:\n",
        "            recent_returns = np.zeros(self.n_stocks)\n",
        "        obs.extend(recent_returns)\n",
        "        \n",
        "        # Recent volatility (5-day rolling)\n",
        "        if self.current_step >= 5:\n",
        "            recent_volatility = self.returns_df.iloc[\n",
        "                max(0, self.current_step-5):self.current_step\n",
        "            ].std().values\n",
        "        else:\n",
        "            recent_volatility = np.zeros(self.n_stocks)\n",
        "        obs.extend(recent_volatility)\n",
        "        \n",
        "        return np.array(obs, dtype=np.float32)\n",
        "    \n",
        "    def _execute_action(self, action):\n",
        "        \"\"\"Execute action for V4\"\"\"\n",
        "        # Convert logits to portfolio weights using softmax\n",
        "        action_logits = np.array(action)\n",
        "        exp_logits = np.exp(action_logits - np.max(action_logits))  # Numerical stability\n",
        "        new_weights = exp_logits / np.sum(exp_logits)\n",
        "        \n",
        "        # Calculate transaction costs\n",
        "        weight_change = np.sum(np.abs(new_weights - self.portfolio_weights))\n",
        "        self.last_transaction_cost = weight_change * self.transaction_cost\n",
        "        \n",
        "        # Update weights\n",
        "        self.portfolio_weights = new_weights\n",
        "    \n",
        "    def _calculate_reward(self):\n",
        "        \"\"\"Calculate reward for V4\"\"\"\n",
        "        if self.current_step == 0:\n",
        "            return 0\n",
        "        \n",
        "        # Portfolio return\n",
        "        portfolio_return = self.return_history[-1] if self.return_history else 0\n",
        "        \n",
        "        # Risk-adjusted reward (Sharpe-like)\n",
        "        if len(self.return_history) > 5:\n",
        "            recent_returns = np.array(self.return_history[-5:])\n",
        "            sharpe_ratio = np.mean(recent_returns) / (np.std(recent_returns) + 1e-8)\n",
        "            reward = sharpe_ratio * 0.1\n",
        "        else:\n",
        "            reward = portfolio_return\n",
        "        \n",
        "        # Subtract transaction costs\n",
        "        transaction_penalty = getattr(self, 'last_transaction_cost', 0)\n",
        "        \n",
        "        return reward - transaction_penalty\n",
        "\n",
        "print(\"Action space prototypes V3 and V4 created successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Reward Function Prototypes\n",
        "\n",
        "Let's test different reward functions to see which works best for portfolio optimization.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class RewardFunctionTester:\n",
        "    \"\"\"Test different reward functions on sample data\"\"\"\n",
        "    \n",
        "    def __init__(self, returns_data):\n",
        "        self.returns_data = returns_data\n",
        "        \n",
        "    def simple_return_reward(self, portfolio_return, transaction_cost=0):\n",
        "        \"\"\"Reward 1: Simple portfolio return\"\"\"\n",
        "        return portfolio_return - transaction_cost\n",
        "    \n",
        "    def sharpe_ratio_reward(self, returns_history, transaction_cost=0):\n",
        "        \"\"\"Reward 2: Sharpe ratio-based reward\"\"\"\n",
        "        if len(returns_history) < 2:\n",
        "            return 0\n",
        "        \n",
        "        mean_return = np.mean(returns_history)\n",
        "        std_return = np.std(returns_history)\n",
        "        \n",
        "        if std_return == 0:\n",
        "            return 0\n",
        "        \n",
        "        sharpe = mean_return / std_return\n",
        "        return sharpe * 0.1 - transaction_cost  # Scale down\n",
        "    \n",
        "    def risk_adjusted_reward(self, portfolio_return, portfolio_volatility, transaction_cost=0):\n",
        "        \"\"\"Reward 3: Risk-adjusted return\"\"\"\n",
        "        if portfolio_volatility == 0:\n",
        "            return portfolio_return - transaction_cost\n",
        "        \n",
        "        risk_adjusted = portfolio_return / portfolio_volatility\n",
        "        return risk_adjusted * 0.1 - transaction_cost\n",
        "    \n",
        "    def drawdown_penalty_reward(self, portfolio_return, current_value, peak_value, transaction_cost=0):\n",
        "        \"\"\"Reward 4: Return with drawdown penalty\"\"\"\n",
        "        drawdown = (peak_value - current_value) / peak_value if peak_value > 0 else 0\n",
        "        drawdown_penalty = drawdown * 0.5  # Penalize drawdowns\n",
        "        \n",
        "        return portfolio_return - drawdown_penalty - transaction_cost\n",
        "    \n",
        "    def momentum_reward(self, portfolio_return, momentum_score, transaction_cost=0):\n",
        "        \"\"\"Reward 5: Momentum-based reward\"\"\"\n",
        "        momentum_bonus = momentum_score * 0.1\n",
        "        return portfolio_return + momentum_bonus - transaction_cost\n",
        "    \n",
        "    def test_rewards(self, portfolio_weights, episode_length=100):\n",
        "        \"\"\"Test all reward functions on sample data\"\"\"\n",
        "        results = {}\n",
        "        \n",
        "        # Simulate portfolio performance\n",
        "        start_idx = np.random.randint(0, len(self.returns_data) - episode_length)\n",
        "        sample_returns = self.returns_data.iloc[start_idx:start_idx + episode_length]\n",
        "        \n",
        "        # Calculate portfolio returns\n",
        "        portfolio_returns = []\n",
        "        portfolio_values = [10000]  # Start with $10,000\n",
        "        peak_value = 10000\n",
        "        \n",
        "        for i in range(len(sample_returns)):\n",
        "            daily_return = np.sum(portfolio_weights * sample_returns.iloc[i])\n",
        "            portfolio_returns.append(daily_return)\n",
        "            new_value = portfolio_values[-1] * (1 + daily_return)\n",
        "            portfolio_values.append(new_value)\n",
        "            peak_value = max(peak_value, new_value)\n",
        "        \n",
        "        # Test each reward function\n",
        "        transaction_cost = 0.001\n",
        "        \n",
        "        # Reward 1: Simple return\n",
        "        total_return = (portfolio_values[-1] / portfolio_values[0]) - 1\n",
        "        results['simple_return'] = self.simple_return_reward(total_return, transaction_cost)\n",
        "        \n",
        "        # Reward 2: Sharpe ratio\n",
        "        results['sharpe_ratio'] = self.sharpe_ratio_reward(portfolio_returns, transaction_cost)\n",
        "        \n",
        "        # Reward 3: Risk-adjusted\n",
        "        portfolio_volatility = np.std(portfolio_returns)\n",
        "        results['risk_adjusted'] = self.risk_adjusted_reward(total_return, portfolio_volatility, transaction_cost)\n",
        "        \n",
        "        # Reward 4: Drawdown penalty\n",
        "        results['drawdown_penalty'] = self.drawdown_penalty_reward(\n",
        "            total_return, portfolio_values[-1], peak_value, transaction_cost\n",
        "        )\n",
        "        \n",
        "        # Reward 5: Momentum\n",
        "        momentum_score = np.mean(portfolio_returns[-5:]) if len(portfolio_returns) >= 5 else 0\n",
        "        results['momentum'] = self.momentum_reward(total_return, momentum_score, transaction_cost)\n",
        "        \n",
        "        return results, {\n",
        "            'total_return': total_return,\n",
        "            'volatility': portfolio_volatility,\n",
        "            'sharpe': total_return / portfolio_volatility if portfolio_volatility > 0 else 0,\n",
        "            'max_drawdown': (peak_value - min(portfolio_values)) / peak_value\n",
        "        }\n",
        "\n",
        "# Test reward functions\n",
        "print(\"Testing different reward functions...\")\n",
        "\n",
        "# Create equal weight portfolio for testing\n",
        "equal_weights = np.ones(len(combined_prices.columns)) / len(combined_prices.columns)\n",
        "print(f\"Testing with equal weights: {dict(zip(combined_prices.columns, equal_weights))}\")\n",
        "\n",
        "# Test reward functions\n",
        "reward_tester = RewardFunctionTester(daily_returns)\n",
        "\n",
        "# Run multiple tests\n",
        "test_results = []\n",
        "for i in range(10):  # Test 10 different random periods\n",
        "    rewards, metrics = reward_tester.test_rewards(equal_weights, episode_length=252)\n",
        "    test_results.append({'rewards': rewards, 'metrics': metrics})\n",
        "\n",
        "# Average results\n",
        "avg_rewards = {}\n",
        "avg_metrics = {}\n",
        "\n",
        "for key in test_results[0]['rewards'].keys():\n",
        "    avg_rewards[key] = np.mean([r['rewards'][key] for r in test_results])\n",
        "\n",
        "for key in test_results[0]['metrics'].keys():\n",
        "    avg_metrics[key] = np.mean([r['metrics'][key] for r in test_results])\n",
        "\n",
        "print(\"\\n=== REWARD FUNCTION COMPARISON ===\")\n",
        "print(\"Average reward values:\")\n",
        "for name, value in avg_rewards.items():\n",
        "    print(f\"  {name}: {value:.6f}\")\n",
        "\n",
        "print(\"\\nAverage portfolio metrics:\")\n",
        "for name, value in avg_metrics.items():\n",
        "    print(f\"  {name}: {value:.6f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Environment Validation and Testing\n",
        "\n",
        "Let's test our environment prototypes to ensure they work correctly.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test environment prototypes\n",
        "def test_environment(env_class, env_name, num_episodes=5):\n",
        "    \"\"\"Test an environment prototype\"\"\"\n",
        "    print(f\"\\n=== Testing {env_name} ===\")\n",
        "    \n",
        "    try:\n",
        "        # Create environment\n",
        "        env = env_class(combined_prices, daily_returns, lookback_window=20)\n",
        "        \n",
        "        # Check environment with stable-baselines3\n",
        "        check_env(env)\n",
        "        print(f\"âœ“ {env_name} passes environment check\")\n",
        "        \n",
        "        # Test episodes\n",
        "        episode_returns = []\n",
        "        episode_lengths = []\n",
        "        \n",
        "        for episode in range(num_episodes):\n",
        "            obs, info = env.reset()\n",
        "            episode_reward = 0\n",
        "            step_count = 0\n",
        "            \n",
        "            while True:\n",
        "                # Random action\n",
        "                action = env.action_space.sample()\n",
        "                obs, reward, done, truncated, info = env.step(action)\n",
        "                \n",
        "                episode_reward += reward\n",
        "                step_count += 1\n",
        "                \n",
        "                if done or truncated:\n",
        "                    break\n",
        "            \n",
        "            episode_returns.append(info['total_return'])\n",
        "            episode_lengths.append(step_count)\n",
        "            \n",
        "            print(f\"  Episode {episode+1}: Return={info['total_return']:.4f}, Steps={step_count}\")\n",
        "        \n",
        "        avg_return = np.mean(episode_returns)\n",
        "        avg_length = np.mean(episode_lengths)\n",
        "        \n",
        "        print(f\"âœ“ {env_name} - Avg Return: {avg_return:.4f}, Avg Length: {avg_length:.1f}\")\n",
        "        \n",
        "        return {\n",
        "            'name': env_name,\n",
        "            'avg_return': avg_return,\n",
        "            'avg_length': avg_length,\n",
        "            'episode_returns': episode_returns\n",
        "        }\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"âœ— {env_name} failed: {e}\")\n",
        "        return None\n",
        "\n",
        "# Test all environment prototypes\n",
        "env_results = []\n",
        "\n",
        "# Test V1 (Simple State + Discrete Actions)\n",
        "result = test_environment(PortfolioEnvV1, \"V1 - Simple State + Discrete Actions\")\n",
        "if result:\n",
        "    env_results.append(result)\n",
        "\n",
        "# Test V2 (Technical Indicators + Continuous Actions)\n",
        "result = test_environment(PortfolioEnvV2, \"V2 - Technical Indicators + Continuous Actions\")\n",
        "if result:\n",
        "    env_results.append(result)\n",
        "\n",
        "# Test V3 (Discrete Position Sizing)\n",
        "result = test_environment(PortfolioEnvV3, \"V3 - Discrete Position Sizing\")\n",
        "if result:\n",
        "    env_results.append(result)\n",
        "\n",
        "# Test V4 (Continuous Softmax Actions)\n",
        "result = test_environment(PortfolioEnvV4, \"V4 - Continuous Softmax Actions\")\n",
        "if result:\n",
        "    env_results.append(result)\n",
        "\n",
        "print(\"\\n=== ENVIRONMENT COMPARISON ===\")\n",
        "if env_results:\n",
        "    for result in env_results:\n",
        "        print(f\"{result['name']}: Avg Return = {result['avg_return']:.4f}, Avg Length = {result['avg_length']:.1f}\")\n",
        "else:\n",
        "    print(\"No environments passed testing.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Visualization and Analysis\n",
        "\n",
        "Let's visualize the performance of different environment prototypes.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize environment comparison\n",
        "if env_results:\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "    fig.suptitle('RL Environment Prototypes Comparison', fontsize=16, fontweight='bold')\n",
        "    \n",
        "    # Extract data for plotting\n",
        "    names = [r['name'] for r in env_results]\n",
        "    avg_returns = [r['avg_return'] for r in env_results]\n",
        "    avg_lengths = [r['avg_length'] for r in env_results]\n",
        "    \n",
        "    # Plot 1: Average Returns\n",
        "    axes[0, 0].bar(range(len(names)), avg_returns, alpha=0.7, color='skyblue')\n",
        "    axes[0, 0].set_title('Average Returns by Environment')\n",
        "    axes[0, 0].set_xlabel('Environment')\n",
        "    axes[0, 0].set_ylabel('Average Return')\n",
        "    axes[0, 0].set_xticks(range(len(names)))\n",
        "    axes[0, 0].set_xticklabels([name.split(' - ')[0] for name in names], rotation=45)\n",
        "    axes[0, 0].grid(True, alpha=0.3)\n",
        "    \n",
        "    # Plot 2: Episode Lengths\n",
        "    axes[0, 1].bar(range(len(names)), avg_lengths, alpha=0.7, color='lightcoral')\n",
        "    axes[0, 1].set_title('Average Episode Lengths')\n",
        "    axes[0, 1].set_xlabel('Environment')\n",
        "    axes[0, 1].set_ylabel('Average Steps')\n",
        "    axes[0, 1].set_xticks(range(len(names)))\n",
        "    axes[0, 1].set_xticklabels([name.split(' - ')[0] for name in names], rotation=45)\n",
        "    axes[0, 1].grid(True, alpha=0.3)\n",
        "    \n",
        "    # Plot 3: Return Distribution\n",
        "    for i, result in enumerate(env_results):\n",
        "        axes[1, 0].hist(result['episode_returns'], alpha=0.5, \n",
        "                       label=result['name'].split(' - ')[0], bins=5)\n",
        "    axes[1, 0].set_title('Return Distribution by Environment')\n",
        "    axes[1, 0].set_xlabel('Episode Return')\n",
        "    axes[1, 0].set_ylabel('Frequency')\n",
        "    axes[1, 0].legend()\n",
        "    axes[1, 0].grid(True, alpha=0.3)\n",
        "    \n",
        "    # Plot 4: Return vs Length Scatter\n",
        "    scatter = axes[1, 1].scatter(avg_lengths, avg_returns, s=100, alpha=0.7, c=range(len(names)), cmap='viridis')\n",
        "    axes[1, 1].set_title('Return vs Episode Length')\n",
        "    axes[1, 1].set_xlabel('Average Episode Length')\n",
        "    axes[1, 1].set_ylabel('Average Return')\n",
        "    axes[1, 1].grid(True, alpha=0.3)\n",
        "    \n",
        "    # Add labels to scatter points\n",
        "    for i, name in enumerate(names):\n",
        "        axes[1, 1].annotate(name.split(' - ')[0], (avg_lengths[i], avg_returns[i]), \n",
        "                           xytext=(5, 5), textcoords='offset points', fontsize=8)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # Summary table\n",
        "    print(\"\\n=== ENVIRONMENT PROTOTYPE SUMMARY ===\")\n",
        "    print(f\"{'Environment':<25} {'Avg Return':<12} {'Avg Length':<12} {'Best For':<20}\")\n",
        "    print(\"-\" * 70)\n",
        "    \n",
        "    best_return_idx = np.argmax(avg_returns)\n",
        "    best_length_idx = np.argmax(avg_lengths)\n",
        "    \n",
        "    for i, result in enumerate(env_results):\n",
        "        best_for = []\n",
        "        if i == best_return_idx:\n",
        "            best_for.append(\"Returns\")\n",
        "        if i == best_length_idx:\n",
        "            best_for.append(\"Stability\")\n",
        "        \n",
        "        best_for_str = \", \".join(best_for) if best_for else \"Balanced\"\n",
        "        \n",
        "        print(f\"{result['name']:<25} {result['avg_return']:<12.4f} {result['avg_length']:<12.1f} {best_for_str:<20}\")\n",
        "    \n",
        "else:\n",
        "    print(\"No environment results to visualize.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Recommendations and Next Steps\n",
        "\n",
        "Based on our prototyping results, here are the recommendations for the final RL environment design.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate recommendations based on testing results\n",
        "print(\"=== RL ENVIRONMENT DESIGN RECOMMENDATIONS ===\\n\")\n",
        "\n",
        "print(\"1. STATE SPACE DESIGN:\")\n",
        "print(\"   âœ“ Recommended: Technical indicators + portfolio weights\")\n",
        "print(\"   âœ“ Include: Price ratios, volatility, momentum, RSI\")\n",
        "print(\"   âœ“ Benefits: Rich information for decision making\")\n",
        "print(\"   âœ“ State dimension: ~30 features (6 stocks Ã— 4 indicators + 6 weights)\")\n",
        "\n",
        "print(\"\\n2. ACTION SPACE DESIGN:\")\n",
        "print(\"   âœ“ Recommended: Continuous action space with softmax normalization\")\n",
        "print(\"   âœ“ Benefits: Smooth portfolio weight adjustments\")\n",
        "print(\"   âœ“ Alternative: Discrete position sizing for simpler algorithms\")\n",
        "print(\"   âœ“ Action dimension: 6 (one per stock)\")\n",
        "\n",
        "print(\"\\n3. REWARD FUNCTION DESIGN:\")\n",
        "print(\"   âœ“ Recommended: Sharpe ratio-based reward\")\n",
        "print(\"   âœ“ Include: Transaction cost penalties\")\n",
        "print(\"   âœ“ Benefits: Risk-adjusted performance optimization\")\n",
        "print(\"   âœ“ Formula: Sharpe_ratio Ã— 0.1 - transaction_costs\")\n",
        "\n",
        "print(\"\\n4. ENVIRONMENT PARAMETERS:\")\n",
        "print(\"   âœ“ Episode length: 252 days (1 trading year)\")\n",
        "print(\"   âœ“ Transaction costs: 0.1% per trade\")\n",
        "print(\"   âœ“ Initial capital: $10,000\")\n",
        "print(\"   âœ“ Lookback window: 20 days\")\n",
        "\n",
        "print(\"\\n5. RL ALGORITHM RECOMMENDATIONS:\")\n",
        "print(\"   âœ“ For continuous actions: PPO or SAC\")\n",
        "print(\"   âœ“ For discrete actions: DQN or A3C\")\n",
        "print(\"   âœ“ Start with: PPO (good balance of performance and stability)\")\n",
        "\n",
        "print(\"\\n6. TRAINING STRATEGY:\")\n",
        "print(\"   âœ“ Use time-series splits (no shuffling)\")\n",
        "print(\"   âœ“ Train on 70% of data, validate on 15%, test on 15%\")\n",
        "print(\"   âœ“ Implement early stopping based on validation performance\")\n",
        "print(\"   âœ“ Use multiple random seeds for robustness\")\n",
        "\n",
        "print(\"\\n7. BENCHMARK COMPARISONS:\")\n",
        "print(\"   âœ“ Equal weight portfolio\")\n",
        "print(\"   âœ“ Buy-and-hold strategies\")\n",
        "print(\"   âœ“ Market index (SP500)\")\n",
        "print(\"   âœ“ Traditional portfolio optimization methods\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"NEXT STEPS:\")\n",
        "print(\"1. Implement the recommended environment in src/environment/\")\n",
        "print(\"2. Create training notebook with PPO algorithm\")\n",
        "print(\"3. Implement backtesting framework\")\n",
        "print(\"4. Train and evaluate the model\")\n",
        "print(\"5. Compare against benchmarks\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Save the best performing environment configuration\n",
        "best_env_config = {\n",
        "    'state_space': 'technical_indicators',\n",
        "    'action_space': 'continuous_softmax',\n",
        "    'reward_function': 'sharpe_ratio',\n",
        "    'episode_length': 252,\n",
        "    'transaction_cost': 0.001,\n",
        "    'lookback_window': 20,\n",
        "    'recommended_algorithm': 'PPO'\n",
        "}\n",
        "\n",
        "print(f\"\\nRecommended Environment Configuration:\")\n",
        "for key, value in best_env_config.items():\n",
        "    print(f\"  {key}: {value}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
